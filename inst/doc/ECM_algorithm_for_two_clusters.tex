\documentclass[12pt]{article}
\usepackage{amsmath, amsthm, amssymb}
\setlength{\topmargin}{0.0cm}
\setlength{\textheight}{21.5cm}
\setlength{\oddsidemargin}{0cm} 
\setlength{\textwidth}{16.5cm}
\setlength{\columnsep}{0.6cm}


\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}

\begin{document}
%\SweaveOpts{concordance=TRUE}

\title{ECM algorithm for two clusters, with constraints on the cluster means and variances, and known data variances.}
\author{Roger Day}

\maketitle
We assume that ${Y_k} \mytilde N({\psi _{G(k)}},{\tau _{G(k)k}}){\text{  where  }}{\tau _{G(k)k}} = {\sigma ^2}_k + {V_{G(k)}}$,
and $\Pr [G(k) = g] = \pi _g{\text{ for }}g = 0,1$.

The group membership vector $G$ is  regarded as missing data for purposes of the "expectation/conditional maximization" algorithm (ECM). 

With a normal approximation, the complete data likelihood per observation $k$ is:
\[\begin{gathered}
\Pr [{Y_k},G(k)|\psi ,V,\pi ] = \Pr [{Y_k}|G(k)] \cdot \Pr [G(k)] \hfill \\
\propto \exp ( - {({Y_k} - {\psi _{G(k)}})^2}/2\tau _{G(k)k}^{}) \cdot {(\tau _{G(k)k}^{})^{ - 1/2}} \cdot {\pi _{G(k)}} \hfill \\ 
\end{gathered} \]

Now suppose that the variances $\sigma ^2 _k$   are known (approximately). Define the free variable $\phi  = ({\psi _0},{\psi _1},{\pi _1},{V_0},{V_1})$ , and  ${\pi _0} = 1 - {\pi _1}$, and similarly for the current fixed estimate  ${\phi ^*}$.   Following standard calculations, the expectation of the complete-data log likelihood is

\[\begin{gathered}
  Q(\phi ,\phi *) = {E^*}\left( { - \sum\nolimits_k {\left( {{{({Y_k} - {\psi _{G(k)}})}^2}/2{\tau _{G(k)k}}(V) - 1/2\log {\tau _{G(k)k}}(V)} \right)}  + {N_0}\log {\pi _0} + {N_1}\log {\pi _1}} \right) \hfill \\
   = \raise.5ex\hbox{$\scriptstyle 1$}\kern-.1em/
 \kern-.15em\lower.25ex\hbox{$\scriptstyle 2$} \sum\nolimits_k {\sum\nolimits_g {\pi _{gk}^*\left( { - {{({Y_k} - {\psi _g})}^2}/(\sigma _k^2 + {V_g}) - \log (\sigma _k^2 + {V_g})} \right)} }  + {N^*}_0\log {\pi _0} + {N^*}_1\log {\pi _1} \hfill \\ 
\end{gathered} \]
with
\[\begin{gathered}
\frac{{{\pi ^*}_{0k}}}{{{\pi ^*}_{1k}}} = \frac{{\Pr ({Y_k},G(k) = 0|\psi _0^*,V_0^*,\pi _0^*)}}{{\Pr ({Y_k},G(k) = 1|\psi _1^*,V_1^*,\pi _1^*)}} = \frac{{{\pi ^*}_0}}{{{\pi ^*}_1}}\frac{{\exp \left( { - {{({Y_k} - \psi _0^*)}^2}/2(V_0^* + \sigma _k^2)} \right)/\sqrt {V_0^* + \sigma _k^2} }}{{\exp \left( { - {{({Y_k} - \psi _1^*)}^2}/2(V_1^* + \sigma _k^2)} \right)/\sqrt {V_1^* + \sigma _k^2} }}
\end{gathered} \]
so that ${E^*}{N_g} = \sum\limits_k {\pi _{gk}^*} $  for g=0,1.

To maximize $Q$, we set its partial derivatives to zero.

\begin{equation*}
\partial Q/\partial {\psi _g} =  - \sum\nolimits_k {{\pi ^*}_{gk}\left( {{{({Y_k} - {\psi _g})}^{}}{{\left( {{\sigma ^2}_k + {{\hat V}_g}} \right)}^{ - 1}}} \right)}  = 0
\end{equation*}
and
\begin{equation*}
\partial Q/\partial {V_g} = \frac{1}{2}\sum\nolimits_k {{\pi ^*}_{gk}\left( { + {{({Y_k} - {\psi _g})}^2}{{\left( {{\sigma ^2}_k + {V_g}} \right)}^{ - 2}} - {{\left( {{\sigma ^2}_k + {V_g}} \right)}^{ - 1}}} \right)}  = 0
\end{equation*}

We cannot solve these two equations simultaneously. However, fixing ${V_g} = {\hat V_g}$ we can solve the first:
\begin{equation*}
{\hat \psi _g} = \frac{{\sum\nolimits_k {{\pi ^*}_{gk}{Y_k}^{}{{\left( {{\sigma ^2}_k + {{\hat V}_g}} \right)}^{ - 1}}} }}{{\sum\nolimits_k {{\pi ^*}_{gk}{{\left( {{\sigma ^2}_k + {{\hat V}_g}} \right)}^{ - 1}}} }}
\end{equation*}
and fixing ${\psi _g} = {\hat \psi _g}$ we can solve the second:
\begin{equation*}
\begin{gathered}  \sum\nolimits_k {{\pi ^*}_{gk}\left( {{{({Y_k} - {\psi _g})}^2} - {{\left( {{\sigma ^2}_k + {V_g}} \right)}^{}}} \right)}  = 0 \hfill \\  \sum\nolimits_k {{\pi ^*}_{gk}{{({Y_k} - {\psi _g})}^2}}  = \sum\nolimits_k {{\pi ^*}_{gk}\left( {{\sigma ^2}_k + {V_g}} \right)}  = \sum\nolimits_k {{\pi ^*}_{gk}{\sigma ^2}_k + N_g^*{V_g}}  \hfill \\  {{\hat V}_g} = \max \left( {0,\sum\nolimits_k {{\pi ^*}_{gk}{{({Y_k} - {{\hat \psi }_g})}^2}}  - \sum\nolimits_k {{\pi ^*}_{gk}{\sigma ^2}_k} } \right)/N_g^* \hfill \\ \end{gathered} 
\end{equation*}
So in the M step we iterate between these formulas for  ${\hat \psi _g}$ and for ${\hat V_g}$. This constitutes an ECM algorithm.

The ECM context makes it easy to account for constraints.  In a mixture of correlation distributions, it may be assumed that $\psi _{0}=0$ . We might also want to assume  $V_{0}=0$, or at least to test it to see if the variation in the $g$=0 component is due only to noise. These constraints are inserted into the respective formulas.


\end{document}
